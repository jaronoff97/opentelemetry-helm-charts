# Top level field indicating an override for fullname
fullnameOverride: ""
# Top level field indicating an override for the namespace
namespaceOverride: ""
# Top level field indicating an override for the kube cluster version
kubeVersionOverride: ""

# Top level field specifying the name of the cluster
clusterName: test

# Top level field related to the OpenTelemetry Operator
opentelemetry-operator:
  # Field indicating whether the operator is enabled or not
  enabled: false

  # Sub-field for admission webhooks configuration
  admissionWebhooks:
    # Policy for handling failures
    # Setting this allows for an installation of the otel operator at the same time as the collectors.
    failurePolicy: "Ignore"

# This is the default configuration for all collectors generated by the chart.
# Any collectors in the `collectors` are overlayed on top of this configuration.
defaultCollectorConfig:
  enabled: false

  # Name of the collector
  name: "test"

  # Annotations for the collector
  annotations: {}
  #   io.opentelemetry.com/resource: hello

  # Labels for the collector
  labels: {}
  #   app: otc

  # Management state of the collector
  managementState: managed

  # Configuration for cluster role binding
  clusterRoleBinding:
    enabled: true
    clusterRoleName: ""

  # Number of replicas for the collector
  replicas: 1

  # Mode of deployment for the collector
  mode: deployment

  # Service account associated with the collector
  serviceAccount: ""

  # Image details for the collector
  image:
    # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
    repository: otel/opentelemetry-collector-contrib
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
    # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
    digest: ""

  # Upgrade strategy for the collector
  upgradeStrategy: automatic

  # Configuration options for the collector
  config: {}
  # receivers:
  #   otlp:
  #     protocols:
  #       grpc:
  #         endpoint: ${env:MY_POD_IP}:4317
  #       http:
  #         endpoint: ${env:MY_POD_IP}:4318
  # exporters:
  #   otlp:
  #     endpoint: "otel-collector.default:4317"
  #     tls:
  #       insecure: true
  #     sending_queue:
  #       num_consumers: 4
  #       queue_size: 100
  #     retry_on_failure:
  #       enabled: true
  # processors:
  #   batch:
  #   memory_limiter:
  #     # 80% of maximum memory up to 2G
  #     limit_mib: 400
  #     # 25% of limit up to 2G
  #     spike_limit_mib: 100
  #     check_interval: 5s
  # extensions:
  #   zpages: {}
  # service:
  #   extensions: [zpages]
  #   pipelines:
  #     traces:
  #       receivers: [otlp]
  #       processors: [memory_limiter, batch]
  #       exporters: [otlp]

  # Whether to use host network for the collector
  hostNetwork: false

  # Whether to share process namespace for the collector
  shareProcessNamespace: false

  # Priority class name for the collector
  priorityClassName: ""

  # Termination grace period for the collector
  terminationGracePeriodSeconds: 30

  # Resource requests and limits for the collector
  resources:
    requests:
      memory: "64Mi"
      cpu: "250m"
    limits:
      memory: "128Mi"
      cpu: "500m"

  # Node selector for the collector
  nodeSelector: {}
  #   nodeType: worker

  # Arguments for the collector
  args: {}
  #   arg1: value1
  #   arg2: value2

  # Autoscaler configuration for the collector
  autoscaler: {}
  #   minReplicas: 1
  #   maxReplicas: 10
  #   targetCPUUtilization: 50

  # Pod disruption budget for the collector
  podDisruptionBudget: {}
  #   maxUnavailable: 1

  # Security context for the collector
  securityContext: {}
  #   runAsUser: 1000
  #   capabilities:
  #     drop:
  #       - ALL

  # Pod security context for the collector
  podSecurityContext: {}
  #   runAsUser: 1000

  # Annotations for the collector's pods
  podAnnotations: {}
  #   prometheus.io/scrape: "true"

  # Target allocator configuration
  targetAllocator: {}
  # replicas: 1
  # nodeSelector:
  #   nodeType: worker
  # resources:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"
  # allocationStrategy: consistent-hashing
  # filterStrategy: relabel-config
  # serviceAccount: my-service-account
  # image: myregistry/myimage:latest
  # enabled: true
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #         - matchExpressions:
  #             - key: kubernetes.io/e2e-az-name
  #               operator: In
  #               values:
  #                 - e2e-az1
  #                 - e2e-az2
  # # Configuration for Prometheus Custom Resources
  # prometheusCR:
  #   enabled: true
  #   scrapeInterval: 30s
  #   podMonitorSelector:
  #     key1: value1
  #     key2: value2
  #   serviceMonitorSelector:
  #     key1: value1
  #     key2: value2
  # securityContext:
  #   runAsUser: 1000
  #   capabilities:
  #     drop:
  #       - ALL
  # podSecurityContext:
  #   runAsUser: 1000
  # # Topology spread constraints for the target allocator
  # topologySpreadConstraints:
  #   - maxSkew: 1
  #     topologyKey: kubernetes.io/hostname
  #     whenUnsatisfiable: DoNotSchedule
  # # Tolerations for the collector
  # tolerations:
  #   - key: "key"
  #     operator: "Equal"
  #     value: "value"
  #     effect: "NoSchedule"
  # # Environment variables for the target allocator
  # env:
  #   - name: ENV_VAR1
  #     value: value1
  #   - name: ENV_VAR2
  #     value: value2
  # # Observability configuration for the target allocator
  # observability:
  #   metrics:
  #     enableMetrics: true

  # Affinity configuration for the collector
  affinity: {}
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #         - matchExpressions:
  #             - key: kubernetes.io/e2e-az-name
  #               operator: In
  #               values:
  #                 - e2e-az1
  #                 - e2e-az2

  # Lifecycle configuration for the collector
  lifecycle: {}
  #   preStop:
  #     exec:
  #       command:
  #         [
  #           "/bin/sh",
  #           "-c",
  #           "echo Hello from the preStop handler > /dev/termination-log",
  #         ]

  # Liveness probe configuration for the collector
  livenessProbe: {}
  #   initialDelaySeconds: 3
  #   periodSeconds: 5
  #   timeoutSeconds: 2
  #   failureThreshold: 5

  # Observability configuration for the collector
  observability: {}
  #   metrics:
  #     enableMetrics: true

  # Update strategy for the collector
  updateStrategy: {}
  #   type: RollingUpdate

  # Volume mounts for the collector
  volumeMounts: []
  #   - name: data
  #     mountPath: /data

  # Ports configuration for the collector
  # The operator automatically calculates ports for known receivers and exporters
  # Set any custom ports here.
  ports: []
  # - name: http
  #   protocol: TCP
  #   port: 80
  #   targetPort: 8080

  # Environment variables for the collector
  env: []
  # - name: ENV_VAR1
  #   value: value1
  # - name: ENV_VAR2
  #   value: value2

  # Volume claim templates for the collector
  volumeClaimTemplates: []
  # - metadata:
  #     name: storage
  #   spec:
  #     accessModes: ["ReadWriteOnce"]
  #     resources:
  #       requests:
  #         storage: 1Gi

  # Tolerations for the collector
  tolerations: []
  # - key: "key"
  #   operator: "Equal"
  #   value: "value"
  #   effect: "NoSchedule"

  # Volumes for the collector
  volumes: []
  # - name: config-volume
  #   configMap:
  #     name: config

  # Init containers for the collector
  initContainers: []
  # - name: init-nginx
  #   image: nginx

  # Additional containers for the collector
  additionalContainers: []
  # - name: additional-container
  #   image: busybox

  # Topology spread constraints for the collector
  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: kubernetes.io/hostname
  #   whenUnsatisfiable: DoNotSchedule
  #   labelSelector:
  #     matchLabels:
  #       app: my-app

  # Config maps for the collector
  configmaps: []
  # - name: config
  #   mountPath: /etc/config

# Top level field specifying collectors configuration
collectors:
  # a collector dedicated to tracing
  tracing:
    enabled: true
    name: traces
    mode: deployment
    autoscaler:
      minReplicas: 1
      maxReplicas: 3
      targetMemoryUtilization: 70
    resources:
      limits:
        cpu: 250m
        memory: 250Mi
      requests:
        cpu: 250m
        memory: 250Mi
    config:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: "0.0.0.0:4317"
      processors:
        resourcedetection/env:
          detectors: [env]
          timeout: 2s
          override: false
        batch:
          send_batch_size: 1000
          timeout: 1s
          send_batch_max_size: 1500
        k8sattributes:
          passthrough: false
          pod_association:
            - sources:
                - from: resource_attribute
                  name: k8s.pod.uid
            - sources:
                - from: resource_attribute
                  name: k8s.pod.name
                - from: resource_attribute
                  name: k8s.namespace.name
                - from: resource_attribute
                  name: k8s.node.name
            - sources:
                - from: resource_attribute
                  name: k8s.pod.ip
            - sources:
                - from: resource_attribute
                  name: k8s.pod.name
                - from: resource_attribute
                  name: k8s.namespace.name
            - sources:
                - from: connection
          extract:
            labels:
              - tag_name: service.name
                key: app.kubernetes.io/name
                from: pod
              - tag_name: service.name
                key: k8s-app
                from: pod
              - tag_name: k8s.app.instance
                key: app.kubernetes.io/instance
                from: pod
              - tag_name: service.version
                key: app.kubernetes.io/version
                from: pod
              - tag_name: k8s.app.component
                key: app.kubernetes.io/component
                from: pod
            metadata:
              - k8s.namespace.name
              - k8s.pod.name
              - k8s.pod.uid
              - k8s.node.name
              - k8s.pod.start_time
              - k8s.deployment.name
              - k8s.replicaset.name
              - k8s.replicaset.uid
              - k8s.daemonset.name
              - k8s.daemonset.uid
              - k8s.job.name
              - k8s.job.uid
              - k8s.container.name
              - k8s.cronjob.name
              - k8s.statefulset.name
              - k8s.statefulset.uid
              - container.image.tag
              - container.image.name
              - k8s.cluster.uid
      exporters:
        debug: {}
      service:
        pipelines:
          traces:
            receivers: [otlp]
            processors:
              - resourcedetection/env
              - k8sattributes
              - batch
            exporters: [debug]
  # a daemonset collector dedicated to getting infrastructure logs and metrics
  daemon:
    enabled: true
    name: daemon
    resources:
      limits:
        cpu: 100m
        memory: 250Mi
      requests:
        cpu: 100m
        memory: 128Mi
    volumeMounts:
      - name: hostfs
        mountPath: /hostfs
        readOnly: true
        mountPropagation: HostToContainer
      - mountPath: /var/log
        name: varlog
        readOnly: true
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
    volumes:
      - name: hostfs
        hostPath:
          path: /
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
    # A scrape config file to instruct the daemon collector to pull metrics from any matching targets on the same node with
    # prometheus.io/scrape=true
    # This config also scrapes a running node exporter and the kubelet CAdvisor metrics which aren't currently supported.
    scrape_configs_file: "daemon_scrape_configs.yaml"
    config:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318
        kubeletstats:
          collection_interval: "15s"
          auth_type: "serviceAccount"
          insecure_skip_verify: true
          # For this scrape to work, the RBAC must have `nodes/stats` GET access.
          endpoint: "https://${env:K8S_NODE_IP}:10250"
          extra_metadata_labels:
            - container.id
            - k8s.volume.type
          metric_groups:
            - node
            - pod
            - volume
            - container
          k8s_api_config:
            auth_type: serviceAccount
        hostmetrics:
          collection_interval: "30s"
          root_path: /hostfs
          scrapers:
            cpu:
              metrics:
                system.cpu.utilization:
                  enabled: true
            disk: {}
            load: {}
            filesystem:
              metrics:
                system.filesystem.utilization:
                  enabled: true
              exclude_mount_points:
                match_type: regexp
                mount_points:
                  - /dev/.*
                  - /proc/.*
                  - /sys/.*
                  - /run/k3s/containerd/.*
                  - /var/lib/docker/.*
                  - /var/lib/kubelet/.*
                  - /snap/.*
              exclude_fs_types:
                match_type: strict
                fs_types:
                  - autofs
                  - binfmt_misc
                  - bpf
                  - cgroup2
                  - configfs
                  - debugfs
                  - devpts
                  - devtmpfs
                  - fusectl
                  - hugetlbfs
                  - iso9660
                  - mqueue
                  - nsfs
                  - overlay
                  - proc
                  - procfs
                  - pstore
                  - rpc_pipefs
                  - securityfs
                  - selinuxfs
                  - squashfs
                  - sysfs
                  - tracefs
            memory:
              metrics:
                system.memory.utilization:
                  enabled: true
            # paging:
            # processes:
            # process:
            network: {}
        k8s_events: {}
        filelog:
          include:
            - /var/log/pods/*/*/*.log
          start_at: beginning
          include_file_path: true
          include_file_name: false
          operators:
            - type: router
              id: get-format
              routes:
                - output: parser-docker
                  expr: 'body matches "^\\{"'
                - output: parser-crio
                  expr: 'body matches "^[^ Z]+ "'
                - output: parser-containerd
                  expr: 'body matches "^[^ Z]+Z"'
            # Parse CRI-O format
            - type: regex_parser
              id: parser-crio
              regex: "^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
              timestamp:
                parse_from: attributes.time
                layout_type: gotime
                layout: "2006-01-02T15:04:05.999999999Z07:00"
            - type: recombine
              id: crio-recombine
              output: extract_metadata_from_filepath
              combine_field: attributes.log
              source_identifier: attributes["log.file.path"]
              is_last_entry: "attributes.logtag == 'F'"
              combine_with: ""
            # Parse CRI-Containerd format
            - type: regex_parser
              id: parser-containerd
              regex: "^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
              timestamp:
                parse_from: attributes.time
                layout: "%Y-%m-%dT%H:%M:%S.%LZ"
            - type: recombine
              id: containerd-recombine
              output: extract_metadata_from_filepath
              combine_field: attributes.log
              source_identifier: attributes["log.file.path"]
              is_last_entry: "attributes.logtag == 'F'"
              combine_with: ""
            # Parse Docker format
            - type: json_parser
              id: parser-docker
              output: extract_metadata_from_filepath
              timestamp:
                parse_from: attributes.time
                layout: "%Y-%m-%dT%H:%M:%S.%LZ"
            # Extract metadata from file path
            - type: regex_parser
              id: extract_metadata_from_filepath
              regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]+)\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
              parse_from: attributes["log.file.path"]
            # Rename attributes
            - type: move
              from: attributes.stream
              to: attributes["log.iostream"]
            - type: move
              from: attributes.container_name
              to: resource["k8s.container.name"]
            - type: move
              from: attributes.namespace
              to: resource["k8s.namespace.name"]
            - type: move
              from: attributes.pod_name
              to: resource["k8s.pod.name"]
            - type: move
              from: attributes.restart_count
              to: resource["k8s.container.restart_count"]
            - type: move
              from: attributes.uid
              to: resource["k8s.pod.uid"]
            # Clean up log body
            - type: move
              from: attributes.log
              to: body
      processors:
        resourcedetection/env:
          detectors: [env]
          timeout: 2s
          override: false
        batch:
          send_batch_size: 1000
          timeout: 1s
          send_batch_max_size: 1500
        k8sattributes:
          passthrough: false
          pod_association:
            - sources:
                - from: resource_attribute
                  name: k8s.pod.uid
            - sources:
                - from: resource_attribute
                  name: k8s.pod.name
                - from: resource_attribute
                  name: k8s.namespace.name
                - from: resource_attribute
                  name: k8s.node.name
            - sources:
                - from: resource_attribute
                  name: k8s.pod.ip
            - sources:
                - from: resource_attribute
                  name: k8s.pod.name
                - from: resource_attribute
                  name: k8s.namespace.name
            - sources:
                - from: connection
          extract:
            labels:
              - tag_name: service.name
                key: app.kubernetes.io/name
                from: pod
              - tag_name: service.name
                key: k8s-app
                from: pod
              - tag_name: k8s.app.instance
                key: app.kubernetes.io/instance
                from: pod
              - tag_name: service.version
                key: app.kubernetes.io/version
                from: pod
              - tag_name: k8s.app.component
                key: app.kubernetes.io/component
                from: pod
            metadata:
              - k8s.namespace.name
              - k8s.pod.name
              - k8s.pod.uid
              - k8s.node.name
              - k8s.pod.start_time
              - k8s.deployment.name
              - k8s.replicaset.name
              - k8s.replicaset.uid
              - k8s.daemonset.name
              - k8s.daemonset.uid
              - k8s.job.name
              - k8s.job.uid
              - k8s.container.name
              - k8s.cronjob.name
              - k8s.statefulset.name
              - k8s.statefulset.uid
              - container.image.tag
              - container.image.name
              - k8s.cluster.uid

      exporters:
        otlp:
          endpoint: ingest.example.com
          sending_queue:
            enabled: true
            num_consumers: 4
            queue_size: 100

          # Retry settings are optional.
          #
          # Note that while retries are attempted, this component will
          # begin to drop arriving data if the queue is not large
          # enough.
          retry_on_failure:
            # We recommend disabling retries, since while the export is
            # blocked it is likely that arriving spans will drop, and
            # Otherwise, collectors will need substantial additional
            # memory to survive transient failures.  Nevertheless, we
            # recommend a limited retry policy to gracefully occasional
            # failures, paired with a modest queue size.
            #
            # Note there is a persistent storage option inherited from a
            # common collector component.  When persistent storage is
            # configured, the default retry configuration is sensible.
            #
            # For more details on retry and queue settings, please refer to
            # https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/exporterhelper/README.md
            enabled: true
            max_elapsed_time: 60s
          timeout: 30s
        debug: {}

      service:
        pipelines:
          metrics:
            exporters:
              - otlp
              - debug
            processors:
              - resourcedetection/env
              - k8sattributes
              - batch
            receivers:
              - prometheus/file
              - otlp
              - kubeletstats
              - hostmetrics
          logs:
            receivers:
              - k8s_events
              - filelog
            processors:
              - resourcedetection/env
              - k8sattributes
              - batch
            exporters:
              - otlp
              - debug

  # a collector dedicated to pulling cluster stats from the kubernetes API server
  cluster:
    name: cluster-stats
    replicas: 1
    mode: deployment
    enabled: true
    resources:
      limits:
        cpu: 100m
        memory: 500Mi
      requests:
        cpu: 100m
        memory: 500Mi
    config:
      receivers:
        k8s_cluster:
          auth_type: serviceAccount
          collection_interval: 10s
          node_conditions_to_report:
            [Ready, MemoryPressure, DiskPressure, NetworkUnavailable]
          allocatable_types_to_report: [cpu, memory, storage]
        k8s_events:
          auth_type: serviceAccount

      processors:
        k8sattributes:
          passthrough: false
          pod_association:
            - sources:
                - from: resource_attribute
                  name: k8s.pod.uid
            - sources:
                - from: resource_attribute
                  name: k8s.pod.name
                - from: resource_attribute
                  name: k8s.namespace.name
                - from: resource_attribute
                  name: k8s.node.name
            - sources:
                - from: resource_attribute
                  name: k8s.pod.ip
            - sources:
                - from: resource_attribute
                  name: k8s.pod.name
                - from: resource_attribute
                  name: k8s.namespace.name
            - sources:
                - from: connection
          extract:
            labels:
              - tag_name: service.name
                key: app.kubernetes.io/name
                from: pod
              - tag_name: service.name
                key: k8s-app
                from: pod
              - tag_name: k8s.app.instance
                key: app.kubernetes.io/instance
                from: pod
              - tag_name: service.version
                key: app.kubernetes.io/version
                from: pod
              - tag_name: k8s.app.component
                key: app.kubernetes.io/component
                from: pod
            metadata:
              - k8s.namespace.name
              - k8s.pod.name
              - k8s.pod.uid
              - k8s.node.name
              - k8s.pod.start_time
              - k8s.deployment.name
              - k8s.replicaset.name
              - k8s.replicaset.uid
              - k8s.daemonset.name
              - k8s.daemonset.uid
              - k8s.job.name
              - k8s.job.uid
              - k8s.container.name
              - k8s.cronjob.name
              - k8s.statefulset.name
              - k8s.statefulset.uid
              - container.image.tag
              - container.image.name
              - k8s.cluster.uid
        resourcedetection/env:
          detectors: [env]
          timeout: 2s
          override: false
        batch:
          send_batch_size: 1000
          timeout: 1s
          send_batch_max_size: 1500

      exporters:
        debug: {}
        otlp:
          endpoint: ingest.example.com
          sending_queue:
            enabled: true
            num_consumers: 4
            queue_size: 100

          # Retry settings are optional.
          #
          # Note that while retries are attempted, this component will
          # begin to drop arriving data if the queue is not large
          # enough.
          retry_on_failure:
            # We recommend disabling retries, since while the export is
            # blocked it is likely that arriving spans will drop, and
            # Otherwise, collectors will need substantial additional
            # memory to survive transient failures.  Nevertheless, we
            # recommend a limited retry policy to gracefully occasional
            # failures, paired with a modest queue size.
            #
            # Note there is a persistent storage option inherited from a
            # common collector component.  When persistent storage is
            # configured, the default retry configuration is sensible.
            #
            # For more details on retry and queue settings, please refer to
            # https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/exporterhelper/README.md
            enabled: true
            max_elapsed_time: 60s
          timeout: 30s

      service:
        pipelines:
          metrics:
            receivers: [k8s_cluster]
            processors: [resourcedetection/env, k8sattributes, batch]
            exporters: [otlp, debug]

  # a collector for metrics which uses the target allocator to pull prometheus metrics
  # disabled by default, enable this and the related prometheus charts below for a replacement
  # of the kube-prometheus-stack chart
  metrics:
    name: metrics
    enabled: false
    mode: statefulset
    replicas: 3
    autoscaler:
      minReplicas: 3
      maxReplicas: 10
      targetMemoryUtilization: 70
    scrape_configs_file: "kubelet_scrape_configs.yaml"
    targetallocator:
      enabled: true
      allocationStrategy: "consistent-hashing"
      replicas: 2
      prometheusCR:
        enabled: true
    resources:
      limits:
        cpu: 250m
        memory: 500Mi
      requests:
        cpu: 250m
        memory: 500Mi
    config:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: "0.0.0.0:4317"
      processors:
        metricstransform/k8sservicename:
          transforms:
            - include: kube_service_info
              match_type: strict
              action: update
              operations:
                - action: update_label
                  label: service
                  new_label: k8s.service.name
        resourcedetection/env:
          detectors: [env]
          timeout: 2s
          override: false
        k8sattributes:
          passthrough: false
          pod_association:
            - sources:
                - from: resource_attribute
                  name: k8s.pod.name
          extract:
            metadata:
              - k8s.namespace.name
              - k8s.pod.name
              - k8s.pod.uid
              - k8s.node.name
              - k8s.pod.start_time
              - k8s.deployment.name
              - k8s.replicaset.name
              - k8s.replicaset.uid
              - k8s.daemonset.name
              - k8s.daemonset.uid
              - k8s.job.name
              - k8s.job.uid
              - k8s.cronjob.name
              - k8s.statefulset.name
              - k8s.statefulset.uid
              - container.image.tag
              - container.image.name
        batch:
          send_batch_size: 1000
          timeout: 1s
          send_batch_max_size: 1500
        resource:
          attributes:
            - key: lightstep.helm_chart
              value: kube-otel-stack
              action: insert
            - key: job
              from_attribute: service.name
              action: insert

      exporters:
        debug: {}

      service:
        extensions:
          - health_check
        pipelines:
          metrics:
            receivers: [prometheus, otlp]
            processors:
              - resource
              - resourcedetection/env
              - k8sattributes
              - metricstransform/k8sservicename
              - batch
            exporters: [debug]

# Cluster role configuration
clusterRole:
  # Whether the cluster role is enabled or not
  enabled: true

  # Annotations for the cluster role
  annotations: {}

  # Rules for the cluster role
  rules: []

# Instrumentation configuration
instrumentation:
  # Whether instrumentation is enabled or not
  enabled: false

  # Exporter configuration
  exporter:
    endpoint: http://collector:55678

  # Resource configuration
  resource:
    resourceAttributes:
      environment: dev
    addK8sUIDAttributes: true

  # Propagators configuration
  propagators:
    - tracecontext
    - baggage
    - b3
    - b3multi
    - jaeger
    - xray
    - ottrace
    - none

  # Sampler configuration
  sampler:
    type: parentbased_always_on
    argument: "0.25"

  # Environment variables for instrumentation
  env:
    - name: ENV_VAR1
      value: value1
    - name: ENV_VAR2
      value: value2

  # Java agent configuration
  java:
    image: myregistry/java-agent:latest
    volumeLimitSize: 200Mi
    env:
      - name: JAVA_ENV_VAR
        value: java_value
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

  # NodeJS agent configuration
  nodejs:
    image: myregistry/nodejs-agent:latest
    volumeLimitSize: 200Mi
    env:
      - name: NODEJS_ENV_VAR
        value: nodejs_value
    resourceRequirements:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

  # Python agent configuration
  python:
    image: myregistry/python-agent:latest
    volumeLimitSize: 200Mi
    env:
      - name: PYTHON_ENV_VAR
        value: python_value
    resourceRequirements:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

  # .NET agent configuration
  dotnet:
    image: myregistry/dotnet-agent:latest
    volumeLimitSize: 200Mi
    env:
      - name: DOTNET_ENV_VAR
        value: dotnet_value
    resourceRequirements:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

  # Go agent configuration
  go:
    image: myregistry/go-agent:latest
    volumeLimitSize: 200Mi
    env:
      - name: GO_ENV_VAR
        value: go_value
    resourceRequirements:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

  # Apache HTTPd agent configuration
  apacheHttpd:
    image: myregistry/apache-agent:latest
    volumeLimitSize: 200Mi
    env:
      - name: APACHE_ENV_VAR
        value: apache_value
    attrs:
      - name: ATTRIBUTE_VAR
        value: attribute_value
    version: "2.4"
    configPath: "/usr/local/apache2/conf"
    resourceRequirements:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

  # NGINX agent configuration
  nginx:
    image: myregistry/nginx-agent:latest
    volumeLimitSize: 200Mi
    env:
      - name: NGINX_ENV_VAR
        value: nginx_value
    attrs:
      - name: ATTRIBUTE_VAR
        value: attribute_value
    configFile: "/etc/nginx/nginx.conf"
    resourceRequirements:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

# OpAMP bridge configuration
opAMPBridge:
  # Whether OpAMP bridge is enabled or not
  enabled: true

  # Adds `opentelemetry.io/opamp-reporting: true` to all collectors
  addReportingLabel: true
  # Adds `opentelemetry.io/opamp-managed: true` to all collectors
  addManagedLabel: false

  # Endpoint for OpAMP server
  endpoint: http://opamp-server:8080

  # Headers configuration for OpAMP bridge
  headers: {}
  # Authorization: Bearer your_access_token
  # Custom-Header: Custom-Value

  # Capabilities of OpAMP bridge
  capabilities:
    AcceptsOpAMPConnectionSettings: true
    AcceptsOtherConnectionSettings: true
    AcceptsRemoteConfig: true
    AcceptsRestartCommand: true
    ReportsEffectiveConfig: true
    ReportsHealth: true
    ReportsOwnLogs: true
    ReportsOwnMetrics: true
    ReportsOwnTraces: true
    ReportsRemoteConfig: true
    ReportsStatus: true

  # Components allowed for OpAMP bridge
  componentsAllowed: {}
  # receiver:
  #   - otlp
  #   - prometheus
  # processor:
  #   - batch
  #   - memory_limiter
  # exporter:
  #   - prometheusremotewrite

  # Resources configuration for OpAMP bridge
  resources:
    limits:
      cpu: "500m"
      memory: "512Mi"
    requests:
      cpu: "250m"
      memory: "256Mi"

  # Number of replicas for OpAMP bridge
  replicas: 1

  # Security context for OpAMP bridge
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000

  # Pod security context for OpAMP bridge
  podSecurityContext:
    fsGroup: 1000

  # Pod annotations for OpAMP bridge
  podAnnotations: {}
  # prometheus.io/scrape: "true"
  # prometheus.io/port: "8080"

  # Service account for OpAMP bridge
  serviceAccount: ""

  # Image for OpAMP bridge
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-operator/operator-opamp-bridge
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
    # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
    digest: ""

  # Upgrade strategy for OpAMP bridge
  upgradeStrategy: automatic

  # Volume mounts for OpAMP bridge
  volumeMounts: []
  # - name: data
  #   mountPath: /data

  # Ports configuration for OpAMP bridge
  ports: []
  # - name: http
  #   port: 8080
  #   protocol: TCP

  # Environment variables for OpAMP bridge
  env: []
  # - name: ENVIRONMENT
  #   value: production

  # Environment variables from config map for OpAMP bridge
  envFrom: []
  # - configMapRef:
  #     name: opamp-config

  # Tolerations for OpAMP bridge
  tolerations: []
  # - key: "opamp"
  #   operator: "Equal"
  #   value: "true"
  #   effect: "NoSchedule"

  # Volumes for OpAMP bridge
  volumes: []
  # - name: data
  #   emptyDir: {}

  # Whether to use host network for OpAMP bridge
  hostNetwork: false

  # Priority class name for OpAMP bridge
  priorityClassName: ""

  # Affinity configuration for OpAMP bridge
  affinity: {}
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #       - matchExpressions:
  #           - key: opamp
  #             operator: In
  #             values:
  #               - "true"

  # Topology spread constraints for OpAMP bridge
  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: "kubernetes.io/hostname"
  #   whenUnsatisfiable: "DoNotSchedule"
  #   labelSelector:
  #     matchLabels:
  #       opamp: "true"

# Bridge cluster role configuration
bridgeClusterRole:
  # Whether the bridge cluster role is enabled or not
  enabled: true

  # Annotations for the bridge cluster role
  annotations: {}

  # Rules for the bridge cluster role
  rules: []

prometheus:
  # controls whether to install the podmonitor and servicemonitor resources for the target allocator
  customResources:
    enabled: false
  ignoreNamespaceSelectors: false

## Flag to disable all the kubernetes component scrapers
##
kubernetesServiceMonitors:
  enabled: false

## Component scraping the kube api server
##
kubeApiServer:
  enabled: false
  tlsConfig:
    serverName: kubernetes
    insecureSkipVerify: false
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    jobLabel: component
    selector:
      matchLabels:
        component: apiserver
        provider: kubernetes

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings:
      # Drop excessively noisy apiserver buckets.
      - action: drop
        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)
        sourceLabels:
          - __name__
          - le
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels:
    #     - __meta_kubernetes_namespace
    #     - __meta_kubernetes_service_name
    #     - __meta_kubernetes_endpoint_port_name
    #   action: keep
    #   regex: default;kubernetes;https
    # - targetLabel: __address__
    #   replacement: kubernetes.default.svc:443

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping the kubelet and kubelet-hosted cAdvisor
## the configuration for this is currently only in kubelet_scrape_configs.yaml
## This is because kubelet doesn't have a service and can only be scraped manually.
kubelet:
  enabled: false
  namespace: kube-system

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## If true, Prometheus use (respect) labels provided by exporter.
    ##
    honorLabels: true

    ## If true, Prometheus ingests metrics with timestamp provided by exporter. If false, Prometheus ingests metrics with timestamp of scrape.
    ##
    honorTimestamps: true

    ## Enable scraping the kubelet over https. For requirements to enable this see
    ## https://github.com/prometheus-operator/prometheus-operator/issues/926
    ##
    https: true

    ## Enable scraping /metrics/cadvisor from kubelet's service
    ##
    cAdvisor: true

    ## Enable scraping /metrics/probes from kubelet's service
    ##
    probes: true

## Component scraping the kube controller manager
##
kubeControllerManager:
  enabled: false

  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  ## If using kubeControllerManager.endpoints only the port and targetPort are used
  ##
  service:
    enabled: true
    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
    ## of default port in Kubernetes 1.22.
    ##
    port: null
    targetPort: null
    # selector:
    #   component: kube-controller-manager

  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## port: Name of the port the metrics will be scraped from
    ##
    port: http-metrics

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    component: kube-controller-manager

    ## Enable scraping kube-controller-manager over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
    ## If null or unset, the value is determined dynamically based on target Kubernetes version.
    ##
    https: null

    # Skip TLS certificate validation when scraping
    insecureSkipVerify: null

    # Name of the server to use when validating TLS certificate
    serverName: null

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping coreDns. Use either this or kubeDns
##
coreDns:
  enabled: false
  service:
    enabled: true
    port: 9153
    targetPort: 9153
    # selector:
    #   k8s-app: kube-dns
  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## port: Name of the port the metrics will be scraped from
    ##
    port: http-metrics

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    k8s-app: kube-dns

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping kubeDns. Use either this or coreDns
##
kubeDns:
  enabled: false
  service:
    dnsmasq:
      port: 10054
      targetPort: 10054
    skydns:
      port: 10055
      targetPort: 10055
    # selector:
    #   k8s-app: kube-dns
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    k8s-app: kube-dns

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    dnsmasqMetricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    dnsmasqRelabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping etcd
##
kubeEtcd:
  enabled: false

  ## If your etcd is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
  ##
  service:
    enabled: true
    port: 2381
    targetPort: 2381
    # selector:
    #   component: etcd

  ## Configure secure access to the etcd cluster by loading a secret into prometheus and
  ## specifying security configuration below. For example, with a secret named etcd-client-cert
  ##
  ## serviceMonitor:
  ##   scheme: https
  ##   insecureSkipVerify: false
  ##   serverName: localhost
  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
  ##
  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""
    scheme: http
    insecureSkipVerify: false
    serverName: ""
    caFile: ""
    certFile: ""
    keyFile: ""

    ## port: Name of the port the metrics will be scraped from
    ##
    port: http-metrics

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    component: etcd

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping kube scheduler
##
kubeScheduler:
  enabled: false

  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  ## If using kubeScheduler.endpoints only the port and targetPort are used
  ##
  service:
    enabled: true
    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
    ## of default port in Kubernetes 1.23.
    ##
    port: null
    targetPort: null
    # selector:
    #   component: kube-scheduler

  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""
    ## Enable scraping kube-scheduler over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
    ## If null or unset, the value is determined dynamically based on target Kubernetes version.
    ##
    https: null

    ## port: Name of the port the metrics will be scraped from
    ##
    port: http-metrics

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    component: kube-scheduler

    ## Skip TLS certificate validation when scraping
    insecureSkipVerify: null

    ## Name of the server to use when validating TLS certificate
    serverName: null

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping kube proxy
##
kubeProxy:
  enabled: false

  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  service:
    enabled: true
    port: 10249
    targetPort: 10249
    # selector:
    #   k8s-app: kube-proxy

  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## port: Name of the port the metrics will be scraped from
    ##
    port: http-metrics

    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    k8s-app: kube-proxy

    ## Enable scraping kube-proxy over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
    ##
    https: false

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping kube state metrics
##
kubeStateMetrics:
  enabled: false

## Configuration for kube-state-metrics subchart
##
kube-state-metrics:
  namespaceOverride: ""
  rbac:
    create: true
  releaseLabel: true
  prometheus:
    monitor:
      enabled: true

      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.
      ##
      scrapeTimeout: ""

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      # Keep labels from scraped data, overriding server-side labels
      ##
      honorLabels: true

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

  selfMonitor:
    enabled: false

## Deploy node exporter as a daemonset to all nodes
##
nodeExporter:
  enabled: false
  operatingSystems:
    linux:
      enabled: false
    darwin:
      enabled: false

  ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled
  ##
  forceDeployDashboards: false

## Configuration for prometheus-node-exporter subchart
##
prometheus-node-exporter:
  namespaceOverride: ""
  podLabels:
    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
    ##
    jobLabel: node-exporter
  releaseLabel: true
  extraArgs:
    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
  service:
    portName: http-metrics
  prometheus:
    monitor:
      enabled: true

      jobLabel: jobLabel

      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.
      ##
      scrapeTimeout: ""

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+
      #   replacement: $1
      #   action: drop

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
  rbac:
    ## If true, create PSPs for node-exporter
    ##
    pspEnabled: false
